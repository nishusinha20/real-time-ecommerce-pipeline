"""
Batch Ingestion Pipeline
Reads raw data generated by data generators, performs minimal validation/enrichment,
and writes processed data as Parquet files.
"""
from __future__ import annotations

import glob
import json
import os
from dataclasses import dataclass
from datetime import datetime
from typing import List, Dict

import pandas as pd

RAW_DIR = "data/raw"
PROCESSED_DIR = "data/processed"


@dataclass
class IngestionConfig:
    raw_dir: str = RAW_DIR
    processed_dir: str = PROCESSED_DIR


def ensure_dirs(path: str) -> None:
    os.makedirs(path, exist_ok=True)


def read_json_lines(files: List[str]) -> pd.DataFrame:
    records: List[Dict] = []
    for fpath in files:
        with open(fpath, "r") as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                try:
                    records.append(json.loads(line))
                except json.JSONDecodeError:
                    # skip bad lines
                    continue
    return pd.DataFrame.from_records(records) if records else pd.DataFrame()


def read_inventory_csv(files: List[str]) -> pd.DataFrame:
    frames: List[pd.DataFrame] = []
    for fpath in files:
        try:
            frames.append(pd.read_csv(fpath))
        except Exception:
            continue
    return pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()


def minimal_event_validation(df: pd.DataFrame) -> pd.DataFrame:
    if df.empty:
        return df
    # Required columns
    required = ["event_id", "user_id", "event_type", "timestamp"]
    for col in required:
        if col not in df.columns:
            df[col] = pd.NA
    df = df.dropna(subset=["event_id", "user_id", "event_type", "timestamp"], how="any")
    # Parse timestamp
    df["event_ts"] = pd.to_datetime(df["timestamp"], errors="coerce")
    df = df.dropna(subset=["event_ts"])  # drop rows with bad timestamps
    # Partition columns
    df["event_date"] = df["event_ts"].dt.date.astype(str)
    return df


def minimal_transaction_validation(df: pd.DataFrame) -> pd.DataFrame:
    if df.empty:
        return df
    required = ["transaction_id", "user_id", "timestamp", "total_amount"]
    for col in required:
        if col not in df.columns:
            df[col] = pd.NA
    df = df.dropna(subset=["transaction_id", "user_id", "timestamp", "total_amount"], how="any")
    df["txn_ts"] = pd.to_datetime(df["timestamp"], errors="coerce")
    df = df.dropna(subset=["txn_ts"])
    df["txn_date"] = df["txn_ts"].dt.date.astype(str)
    # Ensure numeric
    df["total_amount"] = pd.to_numeric(df["total_amount"], errors="coerce")
    df = df[df["total_amount"] >= 0]
    return df


def minimal_profile_validation(df: pd.DataFrame) -> pd.DataFrame:
    if df.empty:
        return df
    required = ["user_id", "email"]
    for col in required:
        if col not in df.columns:
            df[col] = pd.NA
    df = df.dropna(subset=["user_id", "email"], how="any")
    return df


def write_parquet(df: pd.DataFrame, base_dir: str, dataset: str, partition_cols: List[str] | None = None) -> str:
    if df.empty:
        return ""
    ensure_dirs(base_dir)
    out_dir = os.path.join(base_dir, dataset)
    ensure_dirs(out_dir)
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    out_path = os.path.join(out_dir, f"part-{ts}.parquet")
    # For simplicity with pandas, writing a single parquet file without partitioning
    df.to_parquet(out_path, index=False)
    return out_path


def run_ingestion(cfg: IngestionConfig = IngestionConfig()) -> Dict[str, str]:
    ensure_dirs(cfg.processed_dir)

    # Discover files
    events_files = sorted(glob.glob(os.path.join(cfg.raw_dir, "user_events_*.json")))
    txn_files = sorted(glob.glob(os.path.join(cfg.raw_dir, "transactions_*.json")))
    profile_files = sorted(glob.glob(os.path.join(cfg.raw_dir, "user_profiles_*.json")))
    inventory_files = sorted(glob.glob(os.path.join(cfg.raw_dir, "inventory_*.csv")))

    # Read
    events_df = read_json_lines(events_files)
    txns_df = read_json_lines(txn_files)
    profiles_df = read_json_lines(profile_files)
    inventory_df = read_inventory_csv(inventory_files)

    # Validate/enrich
    events_df = minimal_event_validation(events_df)
    txns_df = minimal_transaction_validation(txns_df)
    profiles_df = minimal_profile_validation(profiles_df)
    # inventory_df: minimal passthrough

    # Write
    outputs: Dict[str, str] = {}
    if not events_df.empty:
        outputs["events"] = write_parquet(events_df, cfg.processed_dir, "events")
    if not txns_df.empty:
        outputs["transactions"] = write_parquet(txns_df, cfg.processed_dir, "transactions")
    if not profiles_df.empty:
        outputs["user_profiles"] = write_parquet(profiles_df, cfg.processed_dir, "user_profiles")
    if not inventory_df.empty:
        outputs["inventory"] = write_parquet(inventory_df, cfg.processed_dir, "inventory")

    return outputs


if __name__ == "__main__":
    outputs = run_ingestion()
    if outputs:
        print("Wrote processed datasets:")
        for name, path in outputs.items():
            print(f"- {name}: {path}")
    else:
        print("No processed outputs were written. Ensure raw data exists under data/raw.")
